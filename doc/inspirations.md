Inspirations
=====

Inspiration comes from such modules as: networkx, nltk, 


Networkx - it shows how it is possible to implement theories into code.
I would argue that implementations of algorithms is a perfect example of 
how scientific computation should be done. If we look into betweenness_centrality,
djiksta and other algorithms. We will see how their implementation is explained.

NLTK - this package is great because it is implementation of many things.
First, it allows download of packages that can be used for further domenstraion.
WE ARE GOING TO DO THE SAME WITH DATASETS THAT WE ARE INTERESTED.
Second, we have to highlight that whole natural language processing tradition is
one of the first communities to enter hardcode it implementations. That is why NLTK
is so great.
We use this inspiration to make one or two more steps from where NLTK now is. NLTK
allows download that are prescribed. We should think of partially subscribed methods
like downloading data from third parties. DBpedia could be a perfect example of this.

We draw some inspirations from https://pypi.python.org/pypi/hypothesis
This package tests the given formulae based on random generation of.
We expect do do similar things in *disciplines*, we will build hypothesis about relations between
Disciplines, Journals, Learned_Societies and other phenomenon. And we would expect pointers and gaps 
to appear.

Other insipiration comes from testing evolutionary hypotheses. https://pythonhosted.org/ete2/tutorial/tutorial_adaptation.html The modules teaches  us how to run a simulations about different disciplines.
Disciplines evolve, they separate, (Abbot and Stichweh) and they adopt different techniques, methods, people and etc.
In our case we can speak about evolution of disciplines. And, we can model it. The easiet way is to model Abbots approach.