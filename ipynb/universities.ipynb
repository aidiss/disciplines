{
 "metadata": {
  "name": "",
  "signature": "sha256:373b4b4fb84f8519eece64f61151c231674670b7914f1d94fa44e48317a46406"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook is dedicated to getting info about universities.\n",
      "Universities are major structural component of academic institution. Here we will try to find lists of universities and get data about them. We are interested in disciplines that these universities are dealing this. We can guess that from programs, departments, proffessors that university has. We can also guess some things from the projects in which they are involvoved, as well as connections to other universities.\n",
      "- departments\n",
      "- permanent and temporary staf\n",
      "- study programs\n",
      "- research projecets involved in.\n",
      "\n",
      "We will also apply some basing scrapping techniques to look into the keywords of disciplines mentioned in websites."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Univ.cc"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For our first point we have found univ.cc website that contains a lot of universities. We will also get into university rating later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import pandas\n",
      "import time\n",
      "import pickle\n",
      "#print(soup.prettify()) #this shows us the structure of website."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's start by defining a function that would download all the links that we need.\n",
      "In next cell  \n",
      "> all_universities = get_all_universities()\n",
      "\n",
      "is commented, because we do not want to repeat the process we already did."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_all_universities():\n",
      "    all_links = []\n",
      "    try:\n",
      "        for x in range(1, 7229, 50):\n",
      "            payload = {'dom':'world','key':'','start':'{}'.format(x)} #Every twenty numbers\n",
      "            r = requests.get('http://univ.cc/search.php?', params=payload)\n",
      "            soup = BeautifulSoup(r.content)\n",
      "            all_lines = soup.findAll('li')\n",
      "            for x in all_lines:\n",
      "                my_links = x.find('a')\n",
      "                all_links.append(my_links)\n",
      "            time.sleep(0.2)\n",
      "        return all_links\n",
      "    except:\n",
      "        return all_links\n",
      "#all_universities = get_all_universities()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also, let's define some functions that will allow as to save and load data. For now we use pickle to save the data. In the future, we would like to have all addresses at one place."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_txt():\n",
      "    with open('all_univs.txt', 'w') as thefile:\n",
      "        for university_link in all_universities:\n",
      "            thefile.write(\"{}\\n\".format(university_link))\n",
      "\n",
      "def read_txt():\n",
      "    ff = open('all_univs.txt')\n",
      "    recovered = ff.read()\n",
      "    return recovered.split('\\n')\n",
      "all_universities = [x for x in read_txt() if x]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Was our load successful?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(all_universities))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "7216"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to structure our data a bit more."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "university_list = []\n",
      "for s in all_universities:\n",
      "    s1, s2 = s.split('\">')\n",
      "    university_list.append((s1.lstrip('(<a href=)').lstrip('\"'), s2.rstrip('/a>').rstrip('<')))\n",
      "\n",
      "import pandas as pd\n",
      "df = pd.DataFrame(university_list, columns=['url', 'name'])\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>url</th>\n",
        "      <th>name</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>      http://www.uab.ro/</td>\n",
        "      <td> 1 December University of Alba Iulia</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> http://www.smmu.edu.cn/</td>\n",
        "      <td>     2nd Military Medical University</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> http://www.tmmu.edu.cn/</td>\n",
        "      <td>     3rd Military Medical University</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> http://www.fmmu.edu.cn/</td>\n",
        "      <td>     4th Military Medical University</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>       http://www.ah.dk/</td>\n",
        "      <td>            Aalborg Business College</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "                       url                                 name\n",
        "0       http://www.uab.ro/  1 December University of Alba Iulia\n",
        "1  http://www.smmu.edu.cn/      2nd Military Medical University\n",
        "2  http://www.tmmu.edu.cn/      3rd Military Medical University\n",
        "3  http://www.fmmu.edu.cn/      4th Military Medical University\n",
        "4        http://www.ah.dk/             Aalborg Business College"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So now we have a bit more than 7000 thousand links to universities.\n",
      "We can use it in many ways. We will, first of all, be able to look for mentions of disciplines.\n",
      "The best way to do that is to look for disciplines names. We can use web_of_science classifications."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from disciplines.theory import web_of_science_categories\n",
      "discipline_names = web_of_science_categories.categories\n",
      "discipline_names[5:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 71,
       "text": [
        "(u'Agronomy',\n",
        " u'Allergy',\n",
        " u'Anatomy & Morphology',\n",
        " u'Andrology',\n",
        " u'Anesthesiology')"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we will need some change in words, so we can catch not only \"Agronomy\" but also \"Agronomical\" and other forms. NLTK will help us with that. We would also like to get such items as \"anesthesiologist\", plural form included."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import WordNetLemmatizer\n",
      "wnl = WordNetLemmatizer()\n",
      "print(wnl.lemmatize('sociological', 'n'))\n",
      "#Still not working properly"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sociological\n"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But, we will need to have pages to work with. The first way is to download all the pages. The second way is to scrap them as we enter. We chose first option, because it will allow us to work more with the data."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}